\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup[table]{skip=4pt}
\usepackage{framed}
\usepackage{bm}
\usepackage[most]{tcolorbox}


\usepackage{xcolor}
\graphicspath{{img/}} % set of paths to search for images
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\newenvironment{myitemize}
{ \begin{itemize}
		\setlength{\itemsep}{0pt}
		\setlength{\parskip}{0pt}
		\setlength{\parsep}{0pt}     }
	{ \end{itemize}                  }

\usepackage{biblatex} % bibliography
\addbibresource{papers.bib}

\usepackage{tikz}
\usetikzlibrary{positioning,patterns,fit}

\newcommand{\ifans}[1]{\ifanswers \color{red} \textbf{Solution: } #1 \color{black}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\given}{\,|\,}

\newcommand{\safereward}{r_{\text{safe}}}
\newcommand{\lowreward}{\underline{r}_{\text{risk}}}
\newcommand{\highreward}{\overline{r}_{\text{risk}}}
\newcommand{\consreward}{r_{\text{cons}}}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\rhead{\hmwkAuthorName} % Top left header
\lhead{\hmwkClass: \hmwkTitle} % Top center head
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	CODE INCLUSION CONFIGURATION
%----------------------------------------------------------------------------------------

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Python}
\lstset{language=Python,
        frame=single, % Single frame around code
        basicstyle=\footnotesize\ttfamily, % Use small true type font
        keywordstyle=[1]\color{Blue}\bf,
        keywordstyle=[2]\color{Purple},
        keywordstyle=[3]\color{Blue}\underbar, % Custom functions underlined and blue
        identifierstyle=, % Nothing special about identifiers
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
        stringstyle=\color{Purple}, % Strings are purple
        showstringspaces=false, % Don't put marks in string spaces
        tabsize=5, % 5 spaces per tab
        morekeywords={rand},
        morekeywords=[2]{on, off, interp},
        morekeywords=[3]{test},
        morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
        numbers=left, % Line numbers on left
        firstnumber=1, % Line numbers start with line 1
        numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
        stepnumber=5 % Line numbers go in steps of 5
}

\newcommand{\perlscript}[2]{
\begin{itemize}
\item[]\lstinputlisting[caption=#2,label=#1]{#1.pl}
\end{itemize}
}

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment \#3} % Assignment title
\newcommand{\hmwkClass}{CS\ 234} % Course/class
\newcommand{\hmwkAuthorName}{} % Your name

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{-1in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}}
\author{}
\date{} % Insert date here if you want it to appear below your name

\begin{document}

\maketitle
\vspace{-.5in}
\begin{framed}
{\bf Due date: February 24, 2020 at 11:59 PM (23:59) PST}
\\[1em]
These questions require thought, but do not require long answers. Please be as concise as possible.
\\[1em]
We encourage students to discuss in groups for assignments. \textbf{However, each student must finish the
problem set and programming assignment individually, and must turn in her/his assignment.} We ask
that you abide by the university Honor Code and that of the Computer Science department, and make
sure that all of your submitted work is done by yourself. If you have discussed the problems with others,
please include a statement saying who you discussed problems with. Failure to follow these instructions
will be reported to the Office of Community Standards. We reserve the right to run a fraud-detection software on your code.
\\[1em]
Please review any additional instructions posted on the assignment page at
http://cs234.stanford.edu/assignment3. When you are ready to submit, please
follow the instructions on the course website.
\\[1em]
\end{framed}


\section{Policy Gradient Methods (50 pts coding + 15 pts writeup)}
The goal of this problem is to experiment with policy gradient and its variants, including variance reduction methods. Your goals will be to set up policy gradient for both continuous and discrete environments, and implement a neural network baseline for variance reduction. The framework for the policy gradient algorithm is setup in \texttt{main.py}, and everything that you need to implement is in the files \texttt{network-utils.py}, \texttt{policy-network.py} and \texttt{baseline-network.py}. The file has detailed instructions for each implementation task, but an overview of key steps in the algorithm is provided here.
\subsection{REINFORCE}
Recall the policy gradient theorem,
\[ \nabla_\theta J(\theta) = \mathbb E_{\pi_\theta} \left[ \nabla_\theta \log\pi_\theta(a|s) Q^{\pi_\theta} (s,a) \right] \]
REINFORCE is a Monte Carlo policy gradient algorithm, so we will be using the sampled returns $G_t$ as unbiased estimates of $Q^{\pi_\theta}(s,a)$.
Then the gradient update can be expressed as maximizing the following objective function:
\[ J(\theta) = \frac{1}{|D|} \sum_{\tau \in D} \sum_{t=1}^T \log(\pi_\theta(a_t|s_t)) G_t \]
where $D$ is the set of all trajectories collected by policy $\pi_\theta$, and $\tau=(s_0, a_0, r_0,s_1...)$ is a trajectory.

\subsection{Baseline}
One difficulty of training with the REINFORCE algorithm is that the Monte Carlo sampled return(s) $G_t$ can have high variance. To reduce variance, we subtract a baseline $b_{\phi}(s)$ from the estimated returns when computing the policy gradient. A good baseline is the state value function, $V^{\pi_\theta}(s)$, which requires a training update to $\phi$ to minimize the following mean-squared error loss:
\[ L_{MSE} = \frac{1}{|D|} \sum_{\tau \in D} \sum_{t=1}^T (b_{\phi}(s_t) - G_t)^2\]
\subsection{Advantage Normalization}

After subtracting the baseline, we get the following new objective function:

\[ J(\theta) = \frac{1}{|D|} \sum_{\tau \in D} \sum_{t=1}^T \log(\pi_\theta(a_t|s_t)) \hat{A}_t \]

where

\[\hat{A}_t=G_t - b_{\phi}(s_t)\]

A second variance reduction technique is to normalize the computed advantages, $\hat{A}_t$, so that they have mean $0$ and standard deviation $1$. From a theoretical perspective, we can consider centering the advantages to be simply adjusting the advantages by a constant baseline, which does not change the policy gradient. Likewise, rescaling the advantages effectively changes the learning rate by a factor of $1/\sigma$, where $\sigma$ is the standard deviation of the empirical advantages.

\subsection{Coding Questions (50 pts)}
The functions that you need to implement in \textbf{network-utils.py}, \textbf{policy-network.py} and \textbf{baseline-network.py} are enumerated here. Detailed instructions for each function can be found in the comments in each of these files.
\begin{itemize}
\item\texttt{build\_mlp} in network-utils.py
\item\texttt{add\_placeholders\_op} in policy-network.py
\item\texttt{build\_policy\_network\_op} in policy-network.py
\item\texttt{add\_loss\_op} in policy-network.py
\item\texttt{add\_optimizer\_op} in policy-network.py
\item\texttt{get\_returns} in policy-network.py
\item\texttt{normalize\_advantage} in policy-network.py
\item\texttt{add\_baseline\_op} in baseline-network.py
\item\texttt{calculate\_advantage} in baseline-network.py
\item\texttt{update\_baseline} in baseline-network.py
\end{itemize}
\subsection{Writeup Questions (15 pts)}
\begin{enumerate}
\item[(a)(i) (2 pts)] (CartPole-v0)
Test your implementation on the CartPole-v0 environment by running the following command. We are using random seed as 15
\begin{tcolorbox}
\begin{verbatim}
python main.py --env_name cartpole --baseline --r_seed 15
\end{verbatim}
\end{tcolorbox}
With the given configuration file, the average reward should reach $200$ within $100$ iterations. \emph{NOTE: training may repeatedly converge to 200 and diverge. Your plot does not have to reach 200 and stay there. We only require that you achieve a perfect score of 200 sometime during training.}

Include in your writeup the tensorboard plot for the average reward. Start tensorboard with:
\begin{tcolorbox}
\begin{verbatim}
tensorboard --logdir=results
\end{verbatim}
\end{tcolorbox}
and then navigate to the link it gives you. Click on the ``SCALARS'' tab to view the average reward graph.

Now, test your implementation on the CartPole-v0 environment without baseline by running
\begin{tcolorbox}
\begin{verbatim}
python main.py --env_name cartpole --no-baseline --r_seed 15
\end{verbatim}
\end{tcolorbox}
Include the tensorboard plot for the average reward. Do you notice any difference? Explain.

\item[(a)(ii) (2 pts)] (CartPole-v0)
Test your implementation on the CartPole-v0 environment by running the following command. We are using random seed as 12345456
\begin{tcolorbox}
\begin{verbatim}
python main.py --env_name cartpole --baseline --r_seed 12345456
\end{verbatim}
\end{tcolorbox}
With the given configuration file, the average reward should reach $200$ within $100$ iterations. \emph{NOTE: training may repeatedly converge to 200 and diverge. Your plot does not have to reach 200 and stay there. We only require that you achieve a perfect score of 200 sometime during training.}

Include in your writeup the tensorboard plot for the average reward. Start tensorboard with:
\begin{tcolorbox}
\begin{verbatim}
tensorboard --logdir=results
\end{verbatim}
\end{tcolorbox}
and then navigate to the link it gives you. Click on the ``SCALARS'' tab to view the average reward graph.

Now, test your implementation on the CartPole-v0 environment without baseline by running
\begin{tcolorbox}
\begin{verbatim}
python main.py --env_name cartpole --no-baseline --r_seed 12345456
\end{verbatim}
\end{tcolorbox}
Include the tensorboard plot for the average reward. Do you notice any difference? Explain.

\item[(b)(i) (2 pts)](InvertedPendulum-v1)
Test your implementation on the InvertedPendulum-v1 environment by running
\begin{tcolorbox}
\begin{verbatim}
python main.py --env_name pendulum --baseline --r_seed 15
\end{verbatim}
\end{tcolorbox}

With the given configuration file, the average reward should reach $1000$ within $100$ iterations. \emph{NOTE: Again, we only require that you reach 1000 sometime during training.} Include the tensorboard plot for the average reward in your writeup.

Now, test your implementation on the InvertedPendulum-v1 environment without baseline by running for seed 15
\begin{tcolorbox}
\begin{verbatim}
python main.py --env_name pendulum --no-baseline --r_seed 15
\end{verbatim}
\end{tcolorbox}
Include the tensorboard plot for the average reward. Do you notice any difference? Explain.

\item[(b)(ii) (2 pts)](InvertedPendulum-v1)
Test your implementation on the InvertedPendulum-v1 environment by running
\begin{tcolorbox}
\begin{verbatim}
python main.py --env_name pendulum --baseline --r_seed 8
\end{verbatim}
\end{tcolorbox}

With the given configuration file, the average reward should reach $1000$ within $100$ iterations. \emph{NOTE: Again, we only require that you reach 1000 sometime during training.} Include the tensorboard plot for the average reward in your writeup.

Now, test your implementation on the InvertedPendulum-v1 environment without baseline by running for seed 8
\begin{tcolorbox}
\begin{verbatim}
python main.py --env_name pendulum --no-baseline --r_seed 8
\end{verbatim}
\end{tcolorbox}
Include the tensorboard plot for the average reward. Do you notice any difference? Explain.

\item[(c)(i) (2 pts)](HalfCheetah-v1)
Test your implementation on the HalfCheetah-v1 environment with $\gamma = 0.9$ by running the following command for seed 123
\begin{tcolorbox}
\begin{verbatim}
python main.py --env_name cheetah --baseline --r_seed 123
\end{verbatim}
\end{tcolorbox}

With the given configuration file, the average reward should reach $200$ within $100$ iterations. \emph{NOTE: Again, we only require that you reach 200 sometime during training. There is some variance in training. You can run multiple times and report the best results.} Include the tensorboard plot for the average reward in your writeup.

Now, test your implementation on the HalfCheetah-v1 environment without baseline by running for seed 123
\begin{tcolorbox}
\begin{verbatim}
python main.py --env_name cheetah --no-baseline --r_seed 123
\end{verbatim}
\end{tcolorbox}
Include the tensorboard plot for the average reward. Do you notice any difference? Explain.

\item[(c)(ii) (2 pts)](HalfCheetah-v1)
Test your implementation on the HalfCheetah-v1 environment with $\gamma = 0.9$ by running the following command for seed 15
\begin{tcolorbox}
\begin{verbatim}
python main.py --env_name cheetah --baseline --r_seed 15
\end{verbatim}
\end{tcolorbox}

With the given configuration file, the average reward should reach $200$ within $100$ iterations. \emph{NOTE: Again, we only require that you reach 200 sometime during training. There is some variance in training. You can run multiple times and report the best results.} Include the tensorboard plot for the average reward in your writeup.

Now, test your implementation on the HalfCheetah-v1 environment without baseline by running for seed 15
\begin{tcolorbox}
\begin{verbatim}
python main.py --env_name cheetah --no-baseline --r_seed 15
\end{verbatim}
\end{tcolorbox}
Include the tensorboard plot for the average reward. Do you notice any difference? Explain.

\item[(c)(iii) (3 pts)] How do the results differ across seeds? Describe briefly comparing the performance you get for different seeds for the 3 environments. Run the following commands to get the average performance across 3 runs (2 of the previous runs + 1 another run below).

\begin{tcolorbox}
\begin{verbatim}
python main.py --env_name cheetah --no-baseline --r_seed 12345456
\end{verbatim}
\end{tcolorbox}

\begin{tcolorbox}
\begin{verbatim}
python main.py --env_name cheetah --baseline --r_seed 12345456
\end{verbatim}
\end{tcolorbox}

\begin{tcolorbox}
\begin{verbatim}
python plot.py --env HalfCheetah-v1 -d results
\end{verbatim}
\end{tcolorbox}

Please comment on how the averaged performance for HalfCheetah environment  baseline vs. no baseline case.

\subsection{Testing}
We have provided some basic tests in the root directory of the starter code to test your implementation. You can run the following command to run the tests and then check if you have the right implementation for individual functions.

\begin{tcolorbox}
\begin{verbatim}
python run_basic_tests.py
\end{verbatim}
\end{tcolorbox}

\end{enumerate}

\newpage

\section{Best Arm Identification in Multi-armed Bandit (35pts)}
In this problem we focus on the bandit setting with rewards bounded in $[0,1]$. A bandit problem can be considered as a finite-horizon MDP with just one state ($|\mathcal{S}| = 1$) and horizon $1$: each episode consists of taking a single action and observing a reward. In the bandit setting -- unlike in standard RL -- there are no delayed rewards, and the action taken does affect the distribution of future states.

Actions are also referred to as ``arms'' in a bandit setting\footnote{The name ``bandit'' comes from slot machines, which are sometimes called ``one-armed bandits''. Here we imagine a slot machine which has several arms one might pull, each with a different (random) payout.}. Here we consider a multi-armed bandit, meaning that $1 < |\mathcal{A}| < \infty$. Since there is only one state, a policy is simply a distribution over actions. There are exactly $|\mathcal{A}|$ different deterministic policies. Your goal is to design a simple algorithm to identify a near-optimal arm with high probability.

We recall Hoeffding's inequality: if $X_1,\dots,X_n$ are i.i.d. random variables satisfying $0 \le X_i \le 1$ with probability $1$ for all $i$, $\overline X = \E[X_1] = \dots = \E[X_n]$ is the expected value of the random variables, and $\widehat X = \frac{1}{n} \sum_{i=1}^n X_i$ is the sample mean, then for any $\delta > 0$ we have
\begin{align}
\Pr\Bigg(|\widehat X - \overline X | > \sqrt{\frac{\log(2/\delta)}{2n}}	\Bigg) < \delta.
\end{align}

Assuming that the rewards are bounded in $[0,1]$,
we propose this simple strategy: pull each arm $n_e$ times, and return the action with the highest average payout $\widehat r_a$. The purpose of this exercise is to study the number of samples required to output an arm that is at least $\epsilon$-optimal with high probability.
Intuitively, as $n_e$ increases the empirical average of the payout $\widehat r_a$ converges to its expected value $\overline r_a$ for every action $a$, and so choosing the arm with the highest empirical payout $\widehat r_a$ corresponds to approximately choosing the arm with the highest expected payout $\overline r_a$.

\begin{enumerate}
\item[(a) (15 pts)] We start by bounding the probability of the ``bad event'' in which the empirical mean of some arm differs significantly from its expected return. Starting from Hoeffding's inequality with $n_e$ samples allocated to every action, show that:
\begin{align}
\Pr\Bigg(\exists a \in \mathcal{A} \quad \text{s.t.} \quad |\widehat r_a - \overline r_a | > \sqrt{\frac{\log(2/\delta)}{2n_e}}	\Bigg) < |\mathcal{A}|\delta.
\end{align}
Note that, depending on your derivation, you may come up with a tighter upper bound than $|\mathcal{A}|\delta$. This is also acceptable (as long as you argue that your bound is tighter), but showing the inequality above is sufficient.

\item[(b) (20 pts)] After pulling each arm (action) $n_e$ times our algorithm returns the arm with the highest empirical mean:
\begin{equation}
a^\dagger = \arg\max_{a} \widehat r_a
\end{equation}
Notice that $a^\dagger$ is a random variable.
Let ${a^\star} = \arg\max_a \overline r_{a}$ be the true optimal arm. Suppose that we want our algorithm to return at least an $\epsilon$-optimal arm with probability at least $1-\delta'$, as follows:

\begin{equation}
\Pr \Bigg(\overline r_{a^\dagger} \geq  \overline r_{a^\star} - \epsilon \Bigg) \geq 1-\delta'.
\end{equation}
How many samples are needed to ensure this? Express your result as a function of the number of actions, the required precision $\epsilon$ and the failure probability $\delta'$.
\end{enumerate}

\printbibliography

\end{document}
